{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "aYjcGcpQwwFL"
   },
   "source": [
    "## Data Processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "0WkUFKcpgrbn"
   },
   "source": [
    "We begin by loading data from disk & preprocessing it, to provide input to the encoding layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "VRe3RVN8Aica"
   },
   "outputs": [],
   "source": [
    "import os;\n",
    "# util function to load a file\n",
    "def load_data(path):\n",
    "    input_file = os.path.join(path)\n",
    "    with open(input_file, 'r', encoding='utf-8') as f:\n",
    "        return f.read()\n",
    "      \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 103,
     "resources": {
      "http://localhost:8080/nbextensions/google.colab/files.js": {
       "data": "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",
       "headers": [
        [
         "content-type",
         "application/javascript"
        ]
       ],
       "ok": true,
       "status": 200,
       "status_text": ""
      }
     }
    },
    "colab_type": "code",
    "id": "5_ittVTPxXGH",
    "outputId": "4ea70197-9228-4352-eb4a-a255ed769972"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "     <input type=\"file\" id=\"files-c717824e-93cf-4a49-97af-339d99466635\" name=\"files[]\" multiple disabled />\n",
       "     <output id=\"result-c717824e-93cf-4a49-97af-339d99466635\">\n",
       "      Upload widget is only available when the cell has been executed in the\n",
       "      current browser session. Please rerun this cell to enable.\n",
       "      </output>\n",
       "      <script src=\"/nbextensions/google.colab/files.js\"></script> "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving small_vocab_fr to small_vocab_fr\n",
      "Saving small_vocab_en to small_vocab_en\n"
     ]
    }
   ],
   "source": [
    "from google.colab import files\n",
    "\n",
    "uploaded = files.upload() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "t95dwVon2HIJ"
   },
   "outputs": [],
   "source": [
    "#text sentences, one per line. English in 'small_vocab_en', French in 'small_vocab_fr'\n",
    "source_path = 'small_vocab_en'\n",
    "source_text = load_data(source_path)\n",
    "\n",
    "target_path = 'small_vocab_fr'\n",
    "target_text = load_data(target_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Gao3da5kwwFS"
   },
   "source": [
    "Printing some stats about the dataset, and splitting data for training/testing.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 85
    },
    "colab_type": "code",
    "id": "YrzSFxqd3ySW",
    "outputId": "23739ea6-de8a-4643-d2ea-0efa28d8da84"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset Stats\n",
      "Unique words: 227\n",
      "Number of sentences: 118166\n",
      "Average words / sentence: 13.22173044699829\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "training_source = list();\n",
    "validation_source=list();\n",
    "    \n",
    "training_target = list();\n",
    "validation_target = list();\n",
    "    \n",
    "def print_dataset_stats():\n",
    "    print('Dataset Stats')\n",
    "    sentences = source_text.split('\\n')\n",
    "    target_sentences = target_text.split('\\n')\n",
    "    \n",
    "   \n",
    "    # split data set - 85.8% for training, 14.2% for testing\n",
    "    for i in range(len(sentences)):\n",
    "        if(i %  7 == 0): \n",
    "            validation_target.append(target_sentences[i])\n",
    "            validation_source.append(sentences[i])\n",
    "        else:\n",
    "            training_target.append(target_sentences[i])\n",
    "            training_source.append(sentences[i])\n",
    "\n",
    "        \n",
    "    word_count_arr = [len(s.split()) for s in training_source]\n",
    "    \n",
    "    print('Unique words: {}'.format(len(np.unique(np.array(source_text.split())))))\n",
    "    \n",
    "    print('Number of sentences: {}'.format(len(training_source)))\n",
    "\n",
    "    print('Average words / sentence: {}'.format(np.average(word_count_arr)))\n",
    "    \n",
    "print_dataset_stats()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "y42t-mZRpcec"
   },
   "source": [
    "#### Data preprocessing\n",
    "The following operations are performed on the dataset to make it ready for the encoding layer:\n",
    "\n",
    "\n",
    "*   Create vocabulary tables (look up dictionaries for word -> id & id -> word). \n",
    "*   Preprocess data for the model: convert source & target sentences to a list of integer representations of the sentence's words.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "k0IogU7YKqke"
   },
   "outputs": [],
   "source": [
    "\n",
    "# Create & return vocabulary dicts, mapping int id to word & word to int id.\n",
    "\n",
    "import copy\n",
    "import pickle\n",
    "\n",
    "# special symbols indicating padding, end of sentence, unknown word & start of sentence, respectively\n",
    "\n",
    "SYMBOLS = {'<PAD>': 0, '<EOS>': 1, '<UNK>': 2, '<GO>': 3 }\n",
    "\n",
    "def generate_vocabulary_mappings(text):\n",
    "    word_to_int = copy.copy(SYMBOLS)\n",
    "    \n",
    "    vocab = set(text.split())\n",
    "\n",
    "    for integer, word in enumerate(vocab, len(SYMBOLS)):\n",
    "        word_to_int[word] = integer\n",
    "\n",
    "    int_to_word = {index: word for word, index in word_to_int.items()}\n",
    "    \n",
    "\n",
    "    return word_to_int, int_to_word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "zA862dcyOIvJ"
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Convert sentences to list of word ids\n",
    ":param text_sentences: Array of all text sentences.\n",
    ":param vocab_map: Dictionary from the words to an id\n",
    ":return: A list of processed sentences_to_ids. Append <UNK> if unknown word is encountered.\n",
    "\"\"\"\n",
    "def sentences_to_ids(text_sentences, vocab_map):\n",
    "    source_text_ids = list()\n",
    "   \n",
    "    source_text_sentences = text_sentences.split('\\n')\n",
    "    \n",
    "    for sentence in source_text_sentences:\n",
    "        sentence_ids = list()\n",
    "        for word in sentence.split(' '):\n",
    "            try: \n",
    "                sentence_ids.append(vocab_map[word])\n",
    "            except:\n",
    "                sentence_ids.append(vocab_map['<UNK>'])\n",
    "        source_text_ids.append(sentence_ids)\n",
    "\n",
    "    return source_text_ids\n",
    "    \n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Q2ZoxSrFQkC7"
   },
   "outputs": [],
   "source": [
    "# Preprocess Text Data. This will form the input to the encoding layer. \n",
    "\n",
    "source_text_lc = ('\\n').join(training_source).lower();\n",
    "target_text_lc = ('\\n').join(training_target).lower();\n",
    "\n",
    "source_mapping, reverse_source_mapping = generate_vocabulary_mappings(source_text_lc)\n",
    "target_mapping, reverse_target_mapping = generate_vocabulary_mappings(target_text_lc)\n",
    "\n",
    "source_text_p = sentences_to_ids(source_text_lc, source_mapping)\n",
    "target_text_p = sentences_to_ids(target_text_lc, target_mapping)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "u9livjQiIRd3"
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.python.layers.core import Dense\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ZePyKLKYwwFg"
   },
   "source": [
    "## Building the Sequence to Sequence Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "BqE_f7ArwwFh"
   },
   "source": [
    "The encoding layer of the Sequence to Sequence model.\n",
    "RNN size, number of layers, keep_probability for the dropout layer, source sequence lengths, vocabilary size, embedding size of the source data are the parameters that can be used to build/modify this layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "wadrFuYyfsHB"
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    ":return: tuple (RNN output, RNN state)\n",
    "\"\"\"\n",
    "def encoding_layer(rnn_inputs, nn_size, layer_count, dropout_kp, source_length,\n",
    "                   source_vocab_size, encoding_embedding_size):\n",
    "     \n",
    "    encoder_embedding = tf.contrib.layers.embed_sequence(rnn_inputs,source_vocab_size,encoding_embedding_size)\n",
    "    \n",
    "    multi_rnn_cell = tf.contrib.rnn.MultiRNNCell([tf.contrib.rnn.DropoutWrapper(tf.contrib.rnn.LSTMCell(nn_size), dropout_kp) for _ in range(layer_count) ])\n",
    "    \n",
    "    output, state = tf.nn.dynamic_rnn(multi_rnn_cell, encoder_embedding, sequence_length=source_length, dtype=tf.float32)\n",
    "    \n",
    "    return (output,state)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ql-VRMm_wwFl"
   },
   "source": [
    "The decoding layer of the model. It is composed of two layers & we need different kinds of input data for training/inference phases: \n",
    " - Training layer: Takes in inputs from the encoder's state & uses it for training. In the training phase, the input is provided as embedded target labels. \n",
    " - Inference layer:  Takes in output from previous time step as input, makes predictions. The inputs again need to be embeded and the embedding vector should be shared between two different phases.\n",
    " \n",
    " \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "crKms7MhBybS"
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    ":return: Tuple (Training Output, Inference Output) - (BasicDecoderOutput)\n",
    "\"\"\"\n",
    "def decoding_layer(decoder_input, encoder_state,\n",
    "                   target_length, max_target_length,\n",
    "                   rnn_size,\n",
    "                   num_layers, target_vocab_to_int, target_vocab_size,\n",
    "                   batch_size, keep_prob, embedding_size):\n",
    "   \n",
    "    dec_embeddings=tf.Variable(tf.random_uniform([target_vocab_size, embedding_size]))\n",
    "    dec_embed_input=tf.nn.embedding_lookup(dec_embeddings,decoder_input)\n",
    "    \n",
    "    decoder_cell=tf.contrib.rnn.MultiRNNCell([tf.contrib.rnn.DropoutWrapper(tf.contrib.rnn.LSTMCell(rnn_size),keep_prob) for _ in range(num_layers) ])\n",
    "    \n",
    "    output_layer=Dense(target_vocab_size,kernel_initializer=tf.truncated_normal_initializer(mean=0.0,stddev=0.1))\n",
    "    \n",
    "    # training layer\n",
    "    with tf.variable_scope('decode'):\n",
    "        train_help=tf.contrib.seq2seq.TrainingHelper(inputs=dec_embed_input,sequence_length=target_length)\n",
    "        basic_dec=tf.contrib.seq2seq.BasicDecoder(decoder_cell,train_help,encoder_state,output_layer)\n",
    "        training_decoder_output=tf.contrib.seq2seq.dynamic_decode(basic_dec,maximum_iterations=max_target_length)[0]\n",
    "    \n",
    "    # inference layer\n",
    "    with tf.variable_scope('decode',reuse=True):\n",
    "        start_sentence_id = target_vocab_to_int['<GO>']\n",
    "        end_sentence_id = target_vocab_to_int['<EOS>']\n",
    "        start_tokens=tf.tile(tf.constant([start_sentence_id],dtype=tf.int32),[batch_size])\n",
    "        helper=tf.contrib.seq2seq.GreedyEmbeddingHelper(dec_embeddings,start_tokens,end_sentence_id)\n",
    "        infer_decoder=tf.contrib.seq2seq.BasicDecoder(decoder_cell,helper,encoder_state,output_layer)\n",
    "        inference_decoder_output=tf.contrib.seq2seq.dynamic_decode(infer_decoder,maximum_iterations=max_target_length)[0]\n",
    "    \n",
    "    \n",
    "    return (training_decoder_output,inference_decoder_output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "sVayNedZwwFo"
   },
   "source": [
    "Integrate the above two encoder-decoder layers to build the Sequence to Sequence model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Hz1O4q2qHShe"
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    ":return: Tuple (Training Output, Inference Output) - (BasicDecoderOutput)\n",
    "\"\"\"\n",
    "def sequence_model(input_data, target_data, dropout_keep_prob, batch_size,\n",
    "                  source_length, target_length,\n",
    "                  max_target_sentence_length,\n",
    "                  source_vocab_size, target_vocab_size,\n",
    "                  enc_embed_size, dec_embed_size,\n",
    "                  rnn_size, layer_count, target_mapping):\n",
    "    # prepare input to decoder\n",
    "    target_slice = tf.strided_slice(target_data, [0, 0], [batch_size, -1], [1, 1])\n",
    "    dec_input = tf.concat([tf.fill(\n",
    "        [batch_size, 1], target_mapping['<GO>']), target_slice], 1)\n",
    "  \n",
    "    # get encoder state\n",
    "    _unused ,encoder_state = encoding_layer(input_data, rnn_size, layer_count, dropout_keep_prob, \n",
    "                   source_length, source_vocab_size, \n",
    "                   enc_embed_size)\n",
    "    \n",
    "    # get decoder output\n",
    "    (training_decoder_output,inference_decoder_output)=decoding_layer(dec_input, encoder_state,\n",
    "                   target_length, max_target_sentence_length,\n",
    "                   rnn_size, layer_count, target_mapping, target_vocab_size,\n",
    "                   batch_size, dropout_keep_prob, dec_embed_size)\n",
    "    \n",
    "    return (training_decoder_output,inference_decoder_output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Boz5LZmFwwFu"
   },
   "source": [
    "#### Configure Hyperparameters\n",
    "\n",
    "Define number of layers, batch size, RNN size, embedding size for encoder/decoder, keep probability for the dropout layer, learning rate.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "lVI_x9ugH88M"
   },
   "outputs": [],
   "source": [
    "\n",
    "epochs = 3\n",
    "batch_size = 256\n",
    "\n",
    "rnn_size = 196\n",
    "# Number of Layers\n",
    "num_layers = 1\n",
    "\n",
    "encoding_embedding_size = 196\n",
    "decoding_embedding_size = 196\n",
    "\n",
    "# Dropout Keep Probability\n",
    "keep_probability = 0.75\n",
    "learning_rate = 0.002\n",
    "display_step = 50"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "tmSJG0txU9_k"
   },
   "source": [
    "#### Process & Batch Data\n",
    " We feed the data to the network in batches, while processing each batch such that all sentences have the same length. This is achieved by appending PAD keyword Integer ID to all sentences in the batch, ensuring that all sentence sequences (or int array representations of sentences) are of the same length as the longest sentence in the batch.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "CLmwfwaJKlEp"
   },
   "outputs": [],
   "source": [
    "def add_batch_padding(sentence_batch, pad_id):\n",
    "    max_sentence = max([len(sentence) for sentence in sentence_batch])\n",
    "    return [sentence + [pad_id] * (max_sentence - len(sentence)) for sentence in sentence_batch]\n",
    "   \n",
    "def process_curr_batch(batch, pad_id):\n",
    "    padded_batch = np.array(add_batch_padding(batch, pad_id))\n",
    "    return padded_batch, [len(sentence) for sentence in padded_batch]\n",
    "\n",
    "def get_batches(sources, targets, batch_size, source_pad_id, target_pad_id):\n",
    "    for batch_i in range(0, len(sources)//batch_size):\n",
    "        start_i = batch_i * batch_size\n",
    "\n",
    "        sources_batch = sources[start_i:start_i + batch_size]\n",
    "        targets_batch = targets[start_i:start_i + batch_size]\n",
    "\n",
    "        pad_sources_batch, pad_source_lengths = process_curr_batch(sources_batch, source_pad_id)\n",
    "        pad_targets_batch, pad_targets_lengths = process_curr_batch(targets_batch, target_pad_id)\n",
    "\n",
    "        yield pad_sources_batch, pad_targets_batch, pad_source_lengths, pad_targets_lengths"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Kv15DbBNwwFx"
   },
   "source": [
    "#### Initialize Placeholders and Build the Graph\n",
    "This is where we initialize the graph & define the optimization to be used for training.\n",
    "First, we create TF placeholders/ initialize the required variables. Then, we take the output of the sequence to sequence into tensors for training & inference respectively. We use the AdamOptimizer from tf.train to feed back the gradients to the model.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "2UhHr-u6IAP2"
   },
   "outputs": [],
   "source": [
    "save_path = '/tmp/model.ckpt'\n",
    "train_graph = tf.Graph()\n",
    "with train_graph.as_default():\n",
    "  \n",
    "    input_data = tf.placeholder(tf.int32,[None,None],name='input')\n",
    "    targets = tf.placeholder(tf.int32,[None,None],name='target')\n",
    "    \n",
    "    lr = tf.placeholder(tf.float32,name='learning_rate')\n",
    "    keep_prob = tf.placeholder(tf.float32,name='keep_prob')\n",
    "    \n",
    "    target_length = tf.placeholder(tf.int32,[None,],name='target_length')\n",
    "    max_target_length = tf.reduce_max(target_length,name='max_target_len')\n",
    "    source_length = tf.placeholder(tf.int32,[None,],name='source_length')\n",
    "    \n",
    "    input_shape = tf.shape(input_data)\n",
    "    \n",
    "    reverse_data = tf.reverse(input_data, [-1])\n",
    "    \n",
    "    train_logits, inference_logits = sequence_model(reverse_data, targets,\n",
    "                                                   keep_prob, batch_size,\n",
    "                                                   source_length, target_length,\n",
    "                                                   max_target_length,\n",
    "                                                   len(source_mapping), len(target_mapping),\n",
    "                                                   encoding_embedding_size,\n",
    "                                                   decoding_embedding_size,\n",
    "                                                   rnn_size, num_layers,\n",
    "                                                   target_mapping)\n",
    "\n",
    "\n",
    "    training_logits = tf.identity(train_logits.rnn_output, name='logits')\n",
    "    inference_logits = tf.identity(inference_logits.sample_id, name='predictions')\n",
    "\n",
    "    masks = tf.sequence_mask(target_length, max_target_length, dtype=tf.float32, name='masks')\n",
    "\n",
    "    with tf.name_scope(\"optimization\"):\n",
    "        # Loss function\n",
    "        cost = tf.contrib.seq2seq.sequence_loss(training_logits, targets, masks)\n",
    "\n",
    "        # Optimizer\n",
    "        optimizer = tf.train.AdamOptimizer(lr)\n",
    "        train_op = optimizer.apply_gradients(optimizer.compute_gradients(cost))\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Eqr8E2kcs6Eu"
   },
   "source": [
    "#### Train the model\n",
    "Now, we train the model with the hyperparameters defined above, tweaking them based on the model output. \n",
    "*tf.contrib.seq2seq.sequence_loss * is used to compute the loss. It calculates the weighted cross-entropy loss for a sequence of logits.\n",
    "<br><br>\n",
    "\n",
    "Here we also compute the accuracy for training & validation steps, using the *BLEU score*. \n",
    "\n",
    "BLEU (bilingual evaluation understudy) is an algorithm for evaluating the quality of text which has been machine-translated from one natural language to another. In other words, it is a metric for evaluating a generated sentence to a reference sentence.\n",
    "A perfect match results in a score of 1.0, whereas a perfect mismatch results in a score of 0.0. \n",
    "\n",
    "To compute the BLEU score, we use the corpus_bleu implementation provided by the Natural Language Toolkit\n",
    "library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "I07A_82R57Rp"
   },
   "outputs": [],
   "source": [
    "def un_pad(myList):\n",
    "    new_list = list()\n",
    "    for s in myList:\n",
    "        new_s = list()\n",
    "        for w in s:\n",
    "            if(w != '<PAD>'):\n",
    "                new_s.append(w)\n",
    "        new_list.append(new_s)\n",
    "    return new_list\n",
    " \n",
    "\n",
    "def un_pad_target(myList):\n",
    "    new_list = list()\n",
    "    for s in myList:\n",
    "        new_s = list()\n",
    "        container_list = list()\n",
    "        for w in s:\n",
    "            if(w != '<PAD>'):\n",
    "                new_s.append(w)\n",
    "        container_list.append(new_s)\n",
    "        new_list.append(container_list)\n",
    "    return new_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "o3uIzejjd065"
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.translate.bleu_score import corpus_bleu\n",
    "\n",
    "def get_accuracy(target, logits, target_int_to_word):\n",
    "    \"\"\"\n",
    "    Calculate accuracy\n",
    "    \"\"\"\n",
    "    a = np.array(target);\n",
    "    a_p = [[target_int_to_word[j] for j in i] for i in a]\n",
    "    b = un_pad_target(a_p)\n",
    "    l_p = [[target_int_to_word[j] for j in i] for i in logits]\n",
    "    l_pp = un_pad(l_p)\n",
    "    bleu = corpus_bleu(b, l_pp)\n",
    "    return bleu\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 493
    },
    "colab_type": "code",
    "id": "QDGPq7lUKx6v",
    "outputId": "4252948f-dbba-46eb-9e87-e1590b8f8d0f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch   0 Batch   50/461 Training Accuracy(BLEU): 0.1383 Validation Accuracy(BLEU): 0.1303, Loss: 1.8518\n",
      "Epoch   0 Batch  100/461 Training Accuracy(BLEU): 0.2357 Validation Accuracy(BLEU): 0.2412, Loss: 0.9703\n",
      "Epoch   0 Batch  150/461 Training Accuracy(BLEU): 0.3280 Validation Accuracy(BLEU): 0.3056, Loss: 0.7900\n",
      "Epoch   0 Batch  200/461 Training Accuracy(BLEU): 0.3973 Validation Accuracy(BLEU): 0.3984, Loss: 0.6529\n",
      "Epoch   0 Batch  250/461 Training Accuracy(BLEU): 0.4866 Validation Accuracy(BLEU): 0.4782, Loss: 0.4926\n",
      "Epoch   0 Batch  300/461 Training Accuracy(BLEU): 0.5708 Validation Accuracy(BLEU): 0.5836, Loss: 0.4457\n",
      "Epoch   0 Batch  350/461 Training Accuracy(BLEU): 0.6900 Validation Accuracy(BLEU): 0.6652, Loss: 0.3716\n",
      "Epoch   0 Batch  400/461 Training Accuracy(BLEU): 0.7528 Validation Accuracy(BLEU): 0.7539, Loss: 0.2692\n",
      "Epoch   0 Batch  450/461 Training Accuracy(BLEU): 0.8318 Validation Accuracy(BLEU): 0.8187, Loss: 0.2192\n",
      "Epoch   1 Batch   50/461 Training Accuracy(BLEU): 0.8972 Validation Accuracy(BLEU): 0.8920, Loss: 0.1597\n",
      "Epoch   1 Batch  100/461 Training Accuracy(BLEU): 0.9092 Validation Accuracy(BLEU): 0.8966, Loss: 0.1273\n",
      "Epoch   1 Batch  150/461 Training Accuracy(BLEU): 0.9193 Validation Accuracy(BLEU): 0.9192, Loss: 0.1078\n",
      "Epoch   1 Batch  200/461 Training Accuracy(BLEU): 0.9268 Validation Accuracy(BLEU): 0.9200, Loss: 0.1033\n",
      "Epoch   1 Batch  250/461 Training Accuracy(BLEU): 0.9428 Validation Accuracy(BLEU): 0.9305, Loss: 0.0704\n",
      "Epoch   1 Batch  300/461 Training Accuracy(BLEU): 0.9334 Validation Accuracy(BLEU): 0.9335, Loss: 0.0810\n",
      "Epoch   1 Batch  350/461 Training Accuracy(BLEU): 0.9447 Validation Accuracy(BLEU): 0.9367, Loss: 0.0706\n",
      "Epoch   1 Batch  400/461 Training Accuracy(BLEU): 0.9546 Validation Accuracy(BLEU): 0.9430, Loss: 0.0589\n",
      "Epoch   1 Batch  450/461 Training Accuracy(BLEU): 0.9421 Validation Accuracy(BLEU): 0.9532, Loss: 0.0666\n",
      "Epoch   2 Batch   50/461 Training Accuracy(BLEU): 0.9658 Validation Accuracy(BLEU): 0.9613, Loss: 0.0502\n",
      "Epoch   2 Batch  100/461 Training Accuracy(BLEU): 0.9598 Validation Accuracy(BLEU): 0.9553, Loss: 0.0476\n",
      "Epoch   2 Batch  150/461 Training Accuracy(BLEU): 0.9620 Validation Accuracy(BLEU): 0.9484, Loss: 0.0433\n",
      "Epoch   2 Batch  200/461 Training Accuracy(BLEU): 0.9562 Validation Accuracy(BLEU): 0.9596, Loss: 0.0463\n",
      "Epoch   2 Batch  250/461 Training Accuracy(BLEU): 0.9694 Validation Accuracy(BLEU): 0.9648, Loss: 0.0325\n",
      "Epoch   2 Batch  300/461 Training Accuracy(BLEU): 0.9633 Validation Accuracy(BLEU): 0.9671, Loss: 0.0472\n",
      "Epoch   2 Batch  350/461 Training Accuracy(BLEU): 0.9615 Validation Accuracy(BLEU): 0.9663, Loss: 0.0418\n",
      "Epoch   2 Batch  400/461 Training Accuracy(BLEU): 0.9680 Validation Accuracy(BLEU): 0.9688, Loss: 0.0330\n",
      "Epoch   2 Batch  450/461 Training Accuracy(BLEU): 0.9527 Validation Accuracy(BLEU): 0.9667, Loss: 0.0427\n",
      "Model Trained and Saved\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# Split data to training and validation sets\n",
    "processed_source_text_sentences = source_text_p;\n",
    "processed_target_text_sentences = target_text_p;\n",
    "\n",
    "train_source = processed_source_text_sentences[batch_size:]\n",
    "train_target = processed_target_text_sentences[batch_size:]\n",
    "\n",
    "valid_source = processed_source_text_sentences[:batch_size]\n",
    "valid_target = processed_target_text_sentences[:batch_size]\n",
    "\n",
    "(valid_sources_batch, valid_targets_batch, valid_sources_lengths, valid_targets_lengths ) = next(get_batches(valid_source,\n",
    "                                                                                                             valid_target,\n",
    "                                                                                                             batch_size,\n",
    "                                                                                                             source_mapping['<PAD>'],\n",
    "                                                                                                             target_mapping['<PAD>']))                                                                                                  \n",
    "with tf.Session(graph=train_graph) as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "\n",
    "    for epoch_i in range(epochs):\n",
    "        for batch_i, (source_batch, target_batch, sources_lengths, targets_lengths) in enumerate(\n",
    "                get_batches(train_source, train_target, batch_size,\n",
    "                            source_mapping['<PAD>'],\n",
    "                            target_mapping['<PAD>'])):\n",
    "\n",
    "            _, loss = sess.run([train_op, cost], {input_data: source_batch,\n",
    "                 targets: target_batch, lr: learning_rate,\n",
    "                 target_length: targets_lengths, source_length: sources_lengths,\n",
    "                 keep_prob: keep_probability})\n",
    "\n",
    "\n",
    "            if batch_i % display_step == 0 and batch_i > 0:\n",
    "\n",
    "                batch_train_logits = sess.run(inference_logits, {input_data: source_batch,\n",
    "                     source_length: sources_lengths, target_length: targets_lengths,\n",
    "                     keep_prob: 1.0})\n",
    "                \n",
    "                batch_valid_logits = sess.run( inference_logits, {input_data: valid_sources_batch,\n",
    "                     source_length: valid_sources_lengths, target_length: valid_targets_lengths,\n",
    "                     keep_prob: 1.0})\n",
    "\n",
    "                train_acc = get_accuracy(target_batch, batch_train_logits, reverse_target_mapping)\n",
    "\n",
    "                valid_acc = get_accuracy(valid_targets_batch, batch_valid_logits, reverse_target_mapping)\n",
    "\n",
    "                print('Epoch {:>3} Batch {:>4}/{} Training Accuracy(BLEU): {:>6.4f} Validation Accuracy(BLEU): {:>6.4f}, Loss: {:>6.4f}'\n",
    "                      .format(epoch_i, batch_i, len(source_text_p) // batch_size, train_acc, valid_acc, loss))\n",
    "\n",
    "    # Save Model\n",
    "    saver = tf.train.Saver()\n",
    "    saver.save(sess, save_path)\n",
    "    print('Model Trained and Saved')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "xytQPm2SuzCu"
   },
   "source": [
    "## Evaluate Model\n",
    "Now that the model is saved after the training, we calculate the model's accuracy by feeding it the test data we split from the dataset above. This data is never seen by the model during training & validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "wbP-R_g97ppl"
   },
   "outputs": [],
   "source": [
    "def sentence_to_seq(sentence, mapping):\n",
    "    \"\"\"\n",
    "    Convert a sentence to a sequence of ids\n",
    "    :return: List of word ids\n",
    "    \"\"\"\n",
    "    sentence_lower=sentence.lower()\n",
    "    sequence=[]\n",
    "    for word in sentence_lower.split():\n",
    "        if word in mapping:\n",
    "            sequence.append(mapping[word])\n",
    "        else:\n",
    "            sequence.append(mapping['<UNK>'])\n",
    "    \n",
    "    \n",
    "    return sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "AJmhOV-hbb5k"
   },
   "outputs": [],
   "source": [
    "def validate_batch(translate_sentences):\n",
    "  load_path = '/tmp/model.ckpt'\n",
    "  loaded_graph = tf.Graph()\n",
    "  with tf.Session(graph=loaded_graph) as sess:\n",
    "    \n",
    "    \n",
    "      # Load saved model\n",
    "      loader = tf.train.import_meta_graph(load_path + '.meta')\n",
    "      loader.restore(sess, load_path)\n",
    "\n",
    "      input_data = loaded_graph.get_tensor_by_name('input:0')\n",
    "      logits = loaded_graph.get_tensor_by_name('predictions:0')\n",
    "      target_length = loaded_graph.get_tensor_by_name('target_length:0')\n",
    "      source_length = loaded_graph.get_tensor_by_name('source_length:0')\n",
    "      keep_prob = loaded_graph.get_tensor_by_name('keep_prob:0')\n",
    "      \n",
    "      predictions = list()\n",
    "      for sntc in translate_sentences:\n",
    "          translate_sentence = sentence_to_seq(sntc, source_mapping)\n",
    "          translate_logits = sess.run(logits, {input_data: [translate_sentence]*batch_size,\n",
    "                                         target_length: [len(translate_sentence)*2]*batch_size,\n",
    "                                         source_length: [len(translate_sentence)]*batch_size,\n",
    "                                         keep_prob: 1.0})[0]\n",
    "    \n",
    "          translated_list = ['0']*len(translate_logits);\n",
    "          j = 0;\n",
    "          for i in translate_logits:\n",
    "            translated_list[j] = (reverse_target_mapping[i])\n",
    "            j += 1\n",
    "          predictions.append(un_pad([translated_list])[0])\n",
    "  return predictions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1037
    },
    "colab_type": "code",
    "id": "rSyFc2R5dds-",
    "outputId": "46f537fb-d569-4567-c97c-81d06243f755"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from /tmp/model.ckpt\n",
      "CURRENT:  0.903072877861242\n",
      "MEAN:  0.903072877861242\n",
      "INFO:tensorflow:Restoring parameters from /tmp/model.ckpt\n",
      "CURRENT:  0.8925883323898511\n",
      "MEAN:  0.8978306051255466\n",
      "INFO:tensorflow:Restoring parameters from /tmp/model.ckpt\n",
      "CURRENT:  0.8937599081135543\n",
      "MEAN:  0.8964737061215491\n",
      "INFO:tensorflow:Restoring parameters from /tmp/model.ckpt\n",
      "CURRENT:  0.9083818981449657\n",
      "MEAN:  0.8994507541274033\n",
      "INFO:tensorflow:Restoring parameters from /tmp/model.ckpt\n",
      "CURRENT:  0.9042040357490387\n",
      "MEAN:  0.9004014104517303\n",
      "INFO:tensorflow:Restoring parameters from /tmp/model.ckpt\n",
      "CURRENT:  0.8952196649854834\n",
      "MEAN:  0.8995377862073558\n",
      "INFO:tensorflow:Restoring parameters from /tmp/model.ckpt\n",
      "CURRENT:  0.8940507976946362\n",
      "MEAN:  0.8987539307055388\n",
      "INFO:tensorflow:Restoring parameters from /tmp/model.ckpt\n",
      "CURRENT:  0.8916387621432388\n",
      "MEAN:  0.8978645346352513\n",
      "INFO:tensorflow:Restoring parameters from /tmp/model.ckpt\n",
      "CURRENT:  0.8995092218458804\n",
      "MEAN:  0.8980472776586544\n",
      "INFO:tensorflow:Restoring parameters from /tmp/model.ckpt\n",
      "CURRENT:  0.8924211381132086\n",
      "MEAN:  0.8974846637041098\n",
      "INFO:tensorflow:Restoring parameters from /tmp/model.ckpt\n",
      "CURRENT:  0.899067180636664\n",
      "MEAN:  0.8976285288797965\n",
      "INFO:tensorflow:Restoring parameters from /tmp/model.ckpt\n",
      "CURRENT:  0.8931404458184247\n",
      "MEAN:  0.8972545219580156\n",
      "INFO:tensorflow:Restoring parameters from /tmp/model.ckpt\n",
      "CURRENT:  0.8936798442402863\n",
      "MEAN:  0.8969795467489595\n",
      "INFO:tensorflow:Restoring parameters from /tmp/model.ckpt\n",
      "CURRENT:  0.896754911189371\n",
      "MEAN:  0.896963501351846\n",
      "INFO:tensorflow:Restoring parameters from /tmp/model.ckpt\n",
      "CURRENT:  0.8946846488574481\n",
      "MEAN:  0.8968115778522194\n",
      "INFO:tensorflow:Restoring parameters from /tmp/model.ckpt\n",
      "CURRENT:  0.8988613616216453\n",
      "MEAN:  0.8969396893378088\n",
      "INFO:tensorflow:Restoring parameters from /tmp/model.ckpt\n",
      "CURRENT:  0.8896475561903873\n",
      "MEAN:  0.8965107403291369\n",
      "INFO:tensorflow:Restoring parameters from /tmp/model.ckpt\n",
      "CURRENT:  0.8987592709618024\n",
      "MEAN:  0.8966356586976184\n",
      "INFO:tensorflow:Restoring parameters from /tmp/model.ckpt\n",
      "CURRENT:  0.8932679570475469\n",
      "MEAN:  0.8964584112423516\n",
      "INFO:tensorflow:Restoring parameters from /tmp/model.ckpt\n",
      "CURRENT:  0.8915981345825803\n",
      "MEAN:  0.896215397409363\n"
     ]
    }
   ],
   "source": [
    "batchsize = 1000\n",
    "list_bleu_batch = list()\n",
    "for i in range(0, len(validation_source), batchsize):\n",
    "    v_source_batch = validation_source[i:i+batchsize]\n",
    "    v_target_batch = validation_target[i:i+batchsize]\n",
    "    \n",
    "    predictions = validate_batch(v_source_batch)\n",
    "\n",
    "    targ_sentences = list()\n",
    "    for ts in v_target_batch:\n",
    "      targ_sentences.append([ts.split(' ')])\n",
    "\n",
    "    current_bleu = corpus_bleu(targ_sentences, predictions)\n",
    "    print(\"CURRENT: \", current_bleu)\n",
    "    list_bleu_batch.append(current_bleu)\n",
    "    print(\"MEAN: \",  np.mean(np.array(list_bleu_batch)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "lI5M-PZYlsAg"
   },
   "source": [
    "## Accuracy: MEAN BLEU  0.896215397409363"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "MWpoKn9_vW93"
   },
   "source": [
    "## Benchmark Model\n",
    "We use the characted-based sequence to sequence example provided by Keras as our benchmark model. We process and feed it the same data as above. The file en_fr is a tsv of the above dataset, with the first column being the target sentences (in French), and the second column being source (English). This is generated using the fileprocessor.py file provided with this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 904
    },
    "colab_type": "code",
    "id": "CcHQ4jkcNVxi",
    "outputId": "2d173865-27e8-448a-e6a1-e36fe88eee58"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['he dislikes apples , peaches , and grapes .', 'california is usually freezing during december , and it is busy in april .', 'your most feared animal is that shark .', 'paris is usually wet during august , and it is never dry in november .', 'paris is usually beautiful during september , and it is usually snowy in november .', 'the united states is never wet during january , but it is usually hot in october .', 'we like oranges , mangoes , and grapes .', 'they like pears , apples , and mangoes .', 'she dislikes that little red truck .', 'the grapefruit is my most loved fruit , but the banana is her most loved .']\n",
      "['\\til déteste les pommes , les pêches et les raisins .\\n', '\\tla californie est le gel habituellement en décembre , et il est occupé en avril .\\n', '\\tvotre animal le plus redouté est que le requin .\\n', \"\\tparis est généralement humide au mois d' août , et il est jamais sec en novembre .\\n\", '\\tparis est généralement beau en septembre , et il est généralement enneigée en novembre .\\n', '\\tles états-unis est jamais humide en janvier , mais il est généralement chaud en octobre .\\n', '\\tnous aimons les oranges , les mangues et les raisins .\\n', '\\tils aiment les poires , les pommes et les mangues .\\n', '\\telle déteste ce petit camion rouge .\\n', '\\tle pamplemousse est mon fruit le plus cher , mais la banane est la plus aimée .\\n']\n",
      "Number of samples: 110000\n",
      "Number of unique input tokens: 31\n",
      "Number of unique output tokens: 42\n",
      "Max sequence length for inputs: 102\n",
      "Max sequence length for outputs: 116\n",
      "Train on 99000 samples, validate on 11000 samples\n",
      "Epoch 1/20\n",
      "99000/99000 [==============================] - 88s 893us/step - loss: 1.7012 - val_loss: 1.4791\n",
      "Epoch 2/20\n",
      "99000/99000 [==============================] - 85s 856us/step - loss: 1.2778 - val_loss: 1.0178\n",
      "Epoch 3/20\n",
      "99000/99000 [==============================] - 85s 857us/step - loss: 0.8399 - val_loss: 0.6729\n",
      "Epoch 4/20\n",
      "99000/99000 [==============================] - 85s 854us/step - loss: 0.5523 - val_loss: 0.4364\n",
      "Epoch 5/20\n",
      "99000/99000 [==============================] - 85s 858us/step - loss: 0.3696 - val_loss: 0.3138\n",
      "Epoch 6/20\n",
      "99000/99000 [==============================] - 85s 858us/step - loss: 0.2732 - val_loss: 0.2343\n",
      "Epoch 7/20\n",
      "99000/99000 [==============================] - 85s 857us/step - loss: 0.2177 - val_loss: 0.2003\n",
      "Epoch 8/20\n",
      "99000/99000 [==============================] - 85s 855us/step - loss: 0.1882 - val_loss: 0.1765\n",
      "Epoch 9/20\n",
      "99000/99000 [==============================] - 85s 857us/step - loss: 0.1703 - val_loss: 0.1635\n",
      "Epoch 10/20\n",
      "99000/99000 [==============================] - 85s 863us/step - loss: 0.1608 - val_loss: 0.1529\n",
      "Epoch 11/20\n",
      "99000/99000 [==============================] - 85s 858us/step - loss: 0.1543 - val_loss: 0.1491\n",
      "Epoch 12/20\n",
      "99000/99000 [==============================] - 85s 857us/step - loss: 0.1644 - val_loss: 0.1512\n",
      "Epoch 13/20\n",
      "99000/99000 [==============================] - 85s 859us/step - loss: 0.1443 - val_loss: 0.1406\n",
      "Epoch 14/20\n",
      "99000/99000 [==============================] - 85s 860us/step - loss: 0.1383 - val_loss: 0.1369\n",
      "Epoch 15/20\n",
      "99000/99000 [==============================] - 85s 857us/step - loss: 0.1346 - val_loss: 0.1303\n",
      "Epoch 16/20\n",
      "99000/99000 [==============================] - 85s 861us/step - loss: 0.1282 - val_loss: 0.1291\n",
      "Epoch 17/20\n",
      "99000/99000 [==============================] - 85s 861us/step - loss: 0.1240 - val_loss: 0.1221\n",
      "Epoch 18/20\n",
      "99000/99000 [==============================] - 85s 861us/step - loss: 0.1201 - val_loss: 0.1211\n",
      "Epoch 19/20\n",
      "99000/99000 [==============================] - 85s 858us/step - loss: 0.1166 - val_loss: 0.1172\n",
      "Epoch 20/20\n",
      "99000/99000 [==============================] - 85s 859us/step - loss: 0.1165 - val_loss: 0.1165\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/keras/engine/topology.py:2379: UserWarning: Layer lstm_2 was passed non-serializable keyword arguments: {'initial_state': [<tf.Tensor 'lstm_1/while/Exit_2:0' shape=(?, 256) dtype=float32>, <tf.Tensor 'lstm_1/while/Exit_3:0' shape=(?, 256) dtype=float32>]}. They will not be included in the serialized model (and thus will be missing at deserialization time).\n",
      "  str(node.arguments) + '. They will not be included '\n"
     ]
    }
   ],
   "source": [
    "'''Sequence to sequence example in Keras (character-level). \n",
    "Hosted at: https://github.com/keras-team/keras/blob/master/examples/lstm_seq2seq.py\n",
    "\n",
    "# Summary of the algorithm\n",
    "\n",
    "# References\n",
    "\n",
    "- Sequence to Sequence Learning with Neural Networks\n",
    "    https://arxiv.org/abs/1409.3215\n",
    "- Learning Phrase Representations using\n",
    "    RNN Encoder-Decoder for Statistical Machine Translation\n",
    "    https://arxiv.org/abs/1406.1078\n",
    "'''\n",
    "\n",
    "from __future__ import print_function\n",
    "\n",
    "from keras.models import Model\n",
    "from keras.layers import Input, LSTM, Dense\n",
    "import numpy as np\n",
    "\n",
    "batch_size = 1024  # Batch size for training.\n",
    "epochs = 20 # Number of epochs to train for.\n",
    "latent_dim = 256  # Latent dimensionality of the encoding space.\n",
    "num_samples = 110000  # Number of samples to train on.\n",
    "# Path to the data txt file on disk.\n",
    "data_path = 'en_fr'\n",
    "\n",
    "# Vectorize the data.\n",
    "input_texts = []\n",
    "target_texts = []\n",
    "input_characters = set()\n",
    "target_characters = set()\n",
    "with open(data_path, 'r', encoding='utf-8') as f:\n",
    "    lines = f.read().split('\\n')\n",
    "for line in lines[: min(num_samples, len(lines) - 1)]:\n",
    "    target_text, input_text = line.split('\\t')\n",
    "    # We use \"tab\" as the \"start sequence\" character\n",
    "    # for the targets, and \"\\n\" as \"end sequence\" character.\n",
    "    target_text = '\\t' + target_text + '\\n'\n",
    "    input_texts.append(input_text)\n",
    "    target_texts.append(target_text)\n",
    "    for char in input_text:\n",
    "        if char not in input_characters:\n",
    "            input_characters.add(char)\n",
    "    for char in target_text:\n",
    "        if char not in target_characters:\n",
    "            target_characters.add(char)\n",
    "    \n",
    "print(input_texts[30:40])\n",
    "print(target_texts[30:40])\n",
    "\n",
    "input_characters = sorted(list(input_characters))\n",
    "target_characters = sorted(list(target_characters))\n",
    "num_encoder_tokens = len(input_characters)\n",
    "num_decoder_tokens = len(target_characters)\n",
    "max_encoder_seq_length = max([len(txt) for txt in input_texts])\n",
    "max_decoder_seq_length = max([len(txt) for txt in target_texts])\n",
    "\n",
    "print('Number of samples:', len(input_texts))\n",
    "print('Number of unique input tokens:', num_encoder_tokens)\n",
    "print('Number of unique output tokens:', num_decoder_tokens)\n",
    "print('Max sequence length for inputs:', max_encoder_seq_length)\n",
    "print('Max sequence length for outputs:', max_decoder_seq_length)\n",
    "\n",
    "input_token_index = dict(\n",
    "    [(char, i) for i, char in enumerate(input_characters)])\n",
    "target_token_index = dict(\n",
    "    [(char, i) for i, char in enumerate(target_characters)])\n",
    "\n",
    "encoder_input_data = np.zeros(\n",
    "    (len(input_texts), max_encoder_seq_length, num_encoder_tokens),\n",
    "    dtype='float32')\n",
    "decoder_input_data = np.zeros(\n",
    "    (len(input_texts), max_decoder_seq_length, num_decoder_tokens),\n",
    "    dtype='float32')\n",
    "decoder_target_data = np.zeros(\n",
    "    (len(input_texts), max_decoder_seq_length, num_decoder_tokens),\n",
    "    dtype='float32')\n",
    "\n",
    "for i, (input_text, target_text) in enumerate(zip(input_texts, target_texts)):\n",
    "    for t, char in enumerate(input_text):\n",
    "        encoder_input_data[i, t, input_token_index[char]] = 1.\n",
    "    for t, char in enumerate(target_text):\n",
    "        # decoder_target_data is ahead of decoder_input_data by one timestep\n",
    "        decoder_input_data[i, t, target_token_index[char]] = 1.\n",
    "        if t > 0:\n",
    "            # decoder_target_data will be ahead by one timestep\n",
    "            # and will not include the start character.\n",
    "            decoder_target_data[i, t - 1, target_token_index[char]] = 1.\n",
    "\n",
    "# Define an input sequence and process it.\n",
    "encoder_inputs = Input(shape=(None, num_encoder_tokens))\n",
    "encoder = LSTM(latent_dim, return_state=True)\n",
    "encoder_outputs, state_h, state_c = encoder(encoder_inputs)\n",
    "# We discard `encoder_outputs` and only keep the states.\n",
    "encoder_states = [state_h, state_c]\n",
    "\n",
    "# Set up the decoder, using `encoder_states` as initial state.\n",
    "decoder_inputs = Input(shape=(None, num_decoder_tokens))\n",
    "# We set up our decoder to return full output sequences,\n",
    "# and to return internal states as well. We don't use the\n",
    "# return states in the training model, but we will use them in inference.\n",
    "decoder_lstm = LSTM(latent_dim, return_sequences=True, return_state=True)\n",
    "decoder_outputs, _, _ = decoder_lstm(decoder_inputs,\n",
    "                                     initial_state=encoder_states)\n",
    "decoder_dense = Dense(num_decoder_tokens, activation='softmax')\n",
    "decoder_outputs = decoder_dense(decoder_outputs)\n",
    "\n",
    "# Define the model that will turn\n",
    "# `encoder_input_data` & `decoder_input_data` into `decoder_target_data`\n",
    "model = Model([encoder_inputs, decoder_inputs], decoder_outputs)\n",
    "\n",
    "# Run training\n",
    "model.compile(optimizer='rmsprop', loss='categorical_crossentropy')\n",
    "model.fit([encoder_input_data, decoder_input_data], decoder_target_data,\n",
    "          batch_size=batch_size,\n",
    "          epochs=epochs,\n",
    "          validation_split=0.1)\n",
    "# Save model\n",
    "model.save('s2s.h5')\n",
    "\n",
    "# Next: inference mode (sampling).\n",
    "# Here's the drill:\n",
    "# 1) encode input and retrieve initial decoder state\n",
    "# 2) run one step of decoder with this initial state\n",
    "# and a \"start of sequence\" token as target.\n",
    "# Output will be the next target token\n",
    "# 3) Repeat with the current target token and current states\n",
    "\n",
    "# Define sampling models\n",
    "encoder_model = Model(encoder_inputs, encoder_states)\n",
    "\n",
    "decoder_state_input_h = Input(shape=(latent_dim,))\n",
    "decoder_state_input_c = Input(shape=(latent_dim,))\n",
    "decoder_states_inputs = [decoder_state_input_h, decoder_state_input_c]\n",
    "decoder_outputs, state_h, state_c = decoder_lstm(\n",
    "    decoder_inputs, initial_state=decoder_states_inputs)\n",
    "decoder_states = [state_h, state_c]\n",
    "decoder_outputs = decoder_dense(decoder_outputs)\n",
    "decoder_model = Model(\n",
    "    [decoder_inputs] + decoder_states_inputs,\n",
    "    [decoder_outputs] + decoder_states)\n",
    "\n",
    "# Reverse-lookup token index to decode sequences back to\n",
    "# something readable.\n",
    "reverse_input_char_index = dict(\n",
    "    (i, char) for char, i in input_token_index.items())\n",
    "reverse_target_char_index = dict(\n",
    "    (i, char) for char, i in target_token_index.items())\n",
    "\n",
    "\n",
    "def decode_sequence(input_seq):\n",
    "    # Encode the input as state vectors.\n",
    "    states_value = encoder_model.predict(input_seq)\n",
    "\n",
    "    # Generate empty target sequence of length 1.\n",
    "    target_seq = np.zeros((1, 1, num_decoder_tokens))\n",
    "    # Populate the first character of target sequence with the start character.\n",
    "    target_seq[0, 0, target_token_index['\\t']] = 1.\n",
    "\n",
    "    # Sampling loop for a batch of sequences\n",
    "    # (to simplify, here we assume a batch of size 1).\n",
    "    stop_condition = False\n",
    "    decoded_sentence = ''\n",
    "    while not stop_condition:\n",
    "        output_tokens, h, c = decoder_model.predict(\n",
    "            [target_seq] + states_value)\n",
    "\n",
    "        # Sample a token\n",
    "        sampled_token_index = np.argmax(output_tokens[0, -1, :])\n",
    "        sampled_char = reverse_target_char_index[sampled_token_index]\n",
    "        decoded_sentence += sampled_char\n",
    "\n",
    "        # Exit condition: either hit max length\n",
    "        # or find stop character.\n",
    "        if (sampled_char == '\\n' or\n",
    "           len(decoded_sentence) > max_decoder_seq_length):\n",
    "            stop_condition = True\n",
    "\n",
    "        # Update the target sequence (of length 1).\n",
    "        target_seq = np.zeros((1, 1, num_decoder_tokens))\n",
    "        target_seq[0, 0, sampled_token_index] = 1.\n",
    "\n",
    "        # Update states\n",
    "        states_value = [h, c]\n",
    "\n",
    "    return decoded_sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 3417
    },
    "colab_type": "code",
    "id": "Qov1GOxNItKw",
    "outputId": "43b8ebb8-7e23-43b5-ab0d-138161b8d9c0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CORPUS BLEU:  0.17685921733223353\n",
      "CURRENT MEAN BLEU 0.17685921733223353\n",
      "CORPUS BLEU:  0.20364630199448508\n",
      "CURRENT MEAN BLEU 0.1902527596633593\n",
      "CORPUS BLEU:  0.16877319393715523\n",
      "CURRENT MEAN BLEU 0.1830929044212913\n",
      "CORPUS BLEU:  0.1665112331524292\n",
      "CURRENT MEAN BLEU 0.17894748660407578\n",
      "CORPUS BLEU:  0.16994520127282883\n",
      "CURRENT MEAN BLEU 0.1771470295378264\n",
      "CORPUS BLEU:  0.18668272240141323\n",
      "CURRENT MEAN BLEU 0.17873631168175752\n",
      "CORPUS BLEU:  0.191778441237082\n",
      "CURRENT MEAN BLEU 0.18059947304680388\n",
      "CORPUS BLEU:  0.19854238406678962\n",
      "CURRENT MEAN BLEU 0.18284233692430207\n",
      "CORPUS BLEU:  0.1713664300619961\n",
      "CURRENT MEAN BLEU 0.18156723616182363\n",
      "CORPUS BLEU:  0.20759440729110767\n",
      "CURRENT MEAN BLEU 0.18416995327475205\n",
      "CORPUS BLEU:  0.20597848201905886\n",
      "CURRENT MEAN BLEU 0.18615254679696178\n",
      "CORPUS BLEU:  0.20653314090614192\n",
      "CURRENT MEAN BLEU 0.18785092963939343\n",
      "CORPUS BLEU:  0.2050670061759916\n",
      "CURRENT MEAN BLEU 0.18917524321913173\n",
      "CORPUS BLEU:  0.19771951403424023\n",
      "CURRENT MEAN BLEU 0.18978554827735378\n",
      "CORPUS BLEU:  0.21396004706053665\n",
      "CURRENT MEAN BLEU 0.19139718152956595\n",
      "CORPUS BLEU:  0.16152692314330305\n",
      "CURRENT MEAN BLEU 0.18953029038042452\n",
      "CORPUS BLEU:  0.18113209029351549\n",
      "CURRENT MEAN BLEU 0.18903627861060635\n",
      "CORPUS BLEU:  0.1733656667991982\n",
      "CURRENT MEAN BLEU 0.1881656890655281\n",
      "CORPUS BLEU:  0.1948624532442263\n",
      "CURRENT MEAN BLEU 0.18851815033809116\n",
      "CORPUS BLEU:  0.20048918143987718\n",
      "CURRENT MEAN BLEU 0.18911670189318047\n",
      "CORPUS BLEU:  0.20510976619184565\n",
      "CURRENT MEAN BLEU 0.1898782763835931\n",
      "CORPUS BLEU:  0.18261927885226453\n",
      "CURRENT MEAN BLEU 0.1895483219503509\n",
      "CORPUS BLEU:  0.18921139462469624\n",
      "CURRENT MEAN BLEU 0.18953367293619197\n",
      "CORPUS BLEU:  0.2010912613697434\n",
      "CURRENT MEAN BLEU 0.19001523912092333\n",
      "CORPUS BLEU:  0.17586215179128892\n",
      "CURRENT MEAN BLEU 0.18944911562773797\n",
      "CORPUS BLEU:  0.18167178256250072\n",
      "CURRENT MEAN BLEU 0.18914998743292116\n",
      "CORPUS BLEU:  0.19163717403224104\n",
      "CURRENT MEAN BLEU 0.1892421054551182\n",
      "CORPUS BLEU:  0.18544553590749463\n",
      "CURRENT MEAN BLEU 0.18910651368556022\n",
      "CORPUS BLEU:  0.1940361497462179\n",
      "CURRENT MEAN BLEU 0.1892765011359277\n",
      "CORPUS BLEU:  0.2172650825195949\n",
      "CURRENT MEAN BLEU 0.19020945384871663\n",
      "CORPUS BLEU:  0.18639795845463822\n",
      "CURRENT MEAN BLEU 0.1900865023843915\n",
      "CORPUS BLEU:  0.18934547808177185\n",
      "CURRENT MEAN BLEU 0.19006334537493463\n",
      "CORPUS BLEU:  0.1889043932108204\n",
      "CURRENT MEAN BLEU 0.19002822561238572\n",
      "CORPUS BLEU:  0.20316387211252632\n",
      "CURRENT MEAN BLEU 0.19041456815650748\n",
      "CORPUS BLEU:  0.20601727035408526\n",
      "CURRENT MEAN BLEU 0.19086035964786685\n",
      "CORPUS BLEU:  0.18265851653444368\n",
      "CURRENT MEAN BLEU 0.19063253067249397\n",
      "CORPUS BLEU:  0.18723118518140935\n",
      "CURRENT MEAN BLEU 0.1905406024159782\n",
      "CORPUS BLEU:  0.18312384634122977\n",
      "CURRENT MEAN BLEU 0.19034542462453743\n",
      "CORPUS BLEU:  0.20018546347110464\n",
      "CURRENT MEAN BLEU 0.19059773331291097\n",
      "CORPUS BLEU:  0.1907464437840292\n",
      "CURRENT MEAN BLEU 0.1906014510746889\n",
      "CORPUS BLEU:  0.19871258347662937\n",
      "CURRENT MEAN BLEU 0.19079928357229722\n",
      "CORPUS BLEU:  0.1775903821683723\n",
      "CURRENT MEAN BLEU 0.1904847859198228\n",
      "CORPUS BLEU:  0.19255247941491205\n",
      "CURRENT MEAN BLEU 0.19053287181505743\n",
      "CORPUS BLEU:  0.19399050981579216\n",
      "CURRENT MEAN BLEU 0.19061145449689232\n",
      "CORPUS BLEU:  0.20706082046688193\n",
      "CURRENT MEAN BLEU 0.1909769959628921\n",
      "CORPUS BLEU:  0.2031700262355085\n",
      "CURRENT MEAN BLEU 0.19124206183838374\n",
      "CORPUS BLEU:  0.19844789464084475\n",
      "CURRENT MEAN BLEU 0.1913953774299255\n",
      "CORPUS BLEU:  0.16657742040855777\n",
      "CURRENT MEAN BLEU 0.19087833665864698\n",
      "CORPUS BLEU:  0.1832891222843348\n",
      "CURRENT MEAN BLEU 0.1907234547326406\n",
      "CORPUS BLEU:  0.18396313983040047\n",
      "CURRENT MEAN BLEU 0.1905882484345958\n",
      "CORPUS BLEU:  0.17017373194967986\n",
      "CURRENT MEAN BLEU 0.19018796379763664\n",
      "CORPUS BLEU:  0.19241446692818132\n",
      "CURRENT MEAN BLEU 0.19023078116553174\n",
      "CORPUS BLEU:  0.18251910361075896\n",
      "CURRENT MEAN BLEU 0.19008527781544166\n",
      "CORPUS BLEU:  0.21571622104412366\n",
      "CURRENT MEAN BLEU 0.1905599249122691\n",
      "CORPUS BLEU:  0.19613879424352554\n",
      "CURRENT MEAN BLEU 0.19066135890011013\n",
      "CORPUS BLEU:  0.19148236543324892\n",
      "CURRENT MEAN BLEU 0.1906760197310591\n",
      "CORPUS BLEU:  0.18960215700234806\n",
      "CURRENT MEAN BLEU 0.19065718003406418\n",
      "CORPUS BLEU:  0.22264683975131253\n",
      "CURRENT MEAN BLEU 0.19120872589125812\n",
      "CORPUS BLEU:  0.18038821103094826\n",
      "CURRENT MEAN BLEU 0.19102532733430372\n",
      "CORPUS BLEU:  0.1985268519825701\n",
      "CURRENT MEAN BLEU 0.19115035274510817\n",
      "CORPUS BLEU:  0.15971079983055578\n",
      "CURRENT MEAN BLEU 0.19063495023831223\n",
      "CORPUS BLEU:  0.19312423563539283\n",
      "CURRENT MEAN BLEU 0.19067510000278126\n",
      "CORPUS BLEU:  0.1870493666648578\n",
      "CURRENT MEAN BLEU 0.19061754867995712\n",
      "CORPUS BLEU:  0.1637911492054272\n",
      "CURRENT MEAN BLEU 0.19019838618816753\n",
      "CORPUS BLEU:  0.19680368622224767\n",
      "CURRENT MEAN BLEU 0.19030000618869186\n",
      "CORPUS BLEU:  0.1950207813700201\n",
      "CURRENT MEAN BLEU 0.19037153308537866\n",
      "CORPUS BLEU:  0.18351792571339826\n",
      "CURRENT MEAN BLEU 0.19026924043803564\n",
      "CORPUS BLEU:  0.18446199286591416\n",
      "CURRENT MEAN BLEU 0.19018383973844563\n",
      "CORPUS BLEU:  0.18045306919239262\n",
      "CURRENT MEAN BLEU 0.1900428140783579\n",
      "CORPUS BLEU:  0.19364225112775585\n",
      "CURRENT MEAN BLEU 0.190094234607635\n",
      "CORPUS BLEU:  0.1705964983634349\n",
      "CURRENT MEAN BLEU 0.18981961860419558\n",
      "CORPUS BLEU:  0.1875736517255087\n",
      "CURRENT MEAN BLEU 0.18978842461976933\n",
      "CORPUS BLEU:  0.19597525527505336\n",
      "CURRENT MEAN BLEU 0.18987317572463625\n",
      "CORPUS BLEU:  0.20559505384014484\n",
      "CURRENT MEAN BLEU 0.190085633537008\n",
      "CORPUS BLEU:  0.20845238482807515\n",
      "CURRENT MEAN BLEU 0.1903305235542222\n",
      "CORPUS BLEU:  0.20896857839618782\n",
      "CURRENT MEAN BLEU 0.19057576111793229\n",
      "CORPUS BLEU:  0.1625660134436943\n",
      "CURRENT MEAN BLEU 0.190211998161124\n",
      "CORPUS BLEU:  0.18244068042081335\n",
      "CURRENT MEAN BLEU 0.19011236588240207\n",
      "CORPUS BLEU:  0.19225327584784993\n",
      "CURRENT MEAN BLEU 0.19013946600854698\n",
      "CORPUS BLEU:  0.17939383552755706\n",
      "CURRENT MEAN BLEU 0.19000514562753462\n",
      "CORPUS BLEU:  0.199999521092071\n",
      "CURRENT MEAN BLEU 0.19012853297894863\n",
      "CORPUS BLEU:  0.19240511286528175\n",
      "CURRENT MEAN BLEU 0.19015629614829418\n",
      "CORPUS BLEU:  0.16017616298039503\n",
      "CURRENT MEAN BLEU 0.18979508972458456\n",
      "CORPUS BLEU:  0.1825196891553536\n",
      "CURRENT MEAN BLEU 0.1897084778130461\n",
      "CORPUS BLEU:  0.20193403536665555\n",
      "CURRENT MEAN BLEU 0.1898523079019121\n",
      "CORPUS BLEU:  0.19535127127544225\n",
      "CURRENT MEAN BLEU 0.189916249336488\n",
      "CORPUS BLEU:  0.2062617515088933\n",
      "CURRENT MEAN BLEU 0.19010412867180304\n",
      "CORPUS BLEU:  0.2020985456367503\n",
      "CURRENT MEAN BLEU 0.19024042886458653\n",
      "CORPUS BLEU:  0.20044996351967348\n",
      "CURRENT MEAN BLEU 0.1903551427371156\n",
      "CORPUS BLEU:  0.21715041981638378\n",
      "CURRENT MEAN BLEU 0.19065286803799636\n",
      "CORPUS BLEU:  0.17623904869706952\n",
      "CURRENT MEAN BLEU 0.1904944744188653\n",
      "CORPUS BLEU:  0.1670081626564216\n",
      "CURRENT MEAN BLEU 0.19023918842144746\n",
      "CORPUS BLEU:  0.17155354295955466\n",
      "CURRENT MEAN BLEU 0.19003826750250236\n",
      "CORPUS BLEU:  0.1831693558883904\n",
      "CURRENT MEAN BLEU 0.1899651939746927\n",
      "CORPUS BLEU:  0.2193487568081721\n",
      "CURRENT MEAN BLEU 0.19027449463609775\n",
      "CORPUS BLEU:  0.20238497751125156\n",
      "CURRENT MEAN BLEU 0.19040064549938052\n",
      "CORPUS BLEU:  0.21922199332918627\n",
      "CURRENT MEAN BLEU 0.190697772796595\n",
      "CORPUS BLEU:  0.1935849597290239\n",
      "CURRENT MEAN BLEU 0.1907272338877422\n",
      "CORPUS BLEU:  0.20830908930124004\n",
      "CURRENT MEAN BLEU 0.19090482838686845\n",
      "CORPUS BLEU:  0.20374131581366997\n",
      "CURRENT MEAN BLEU 0.19103319326113646\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "benchmark_batchsize = 100\n",
    "bleu_batch_list = list()\n",
    "for index in range(0, 10000, benchmark_batchsize):\n",
    "    input_batch = encoder_input_data[index: index + benchmark_batchsize]\n",
    "    target_batch = target_texts[index: index + benchmark_batchsize]\n",
    "\n",
    "    input_batch_ac = input_texts[index: index + benchmark_batchsize]\n",
    "\n",
    "    targets = list()\n",
    "    predictions = list()\n",
    "    inputs = list()\n",
    "\n",
    "    for seq_index in range(benchmark_batchsize):\n",
    "      # Take one sequence (part of the training set)\n",
    "      # for trying out decoding.\n",
    "        input_seq = input_batch[seq_index: seq_index + 1]\n",
    "        target_text = target_batch[seq_index: seq_index + 1]\n",
    "        input_text = input_batch_ac[seq_index: seq_index + 1]\n",
    "        decoded_sentence = decode_sequence(input_seq)\n",
    "        prediction_s = list()\n",
    "  \n",
    "    for words in decoded_sentence.split('\\n')[0].split(' '):\n",
    "        prediction_s.append(words)\n",
    "      \n",
    "    predictions.append(prediction_s)\n",
    "    \n",
    "    target_s_list = list()\n",
    "    \n",
    "    for t_w in target_text[0].split('\\t')[1].split('\\n')[0].split(' '):\n",
    "        target_s_list.append(t_w)\n",
    "    \n",
    "    targets.append([target_s_list]);\n",
    "    inputs.append(input_text)\n",
    "  \n",
    "  corp_bleu = corpus_bleu(targets, predictions)\n",
    "  print(\"CORPUS BLEU: \", corp_bleu)\n",
    "  bleu_batch_list.append(corp_bleu)\n",
    "  print(\"CURRENT MEAN BLEU\", np.mean(np.array(bleu_batch_list)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "oGYcKf5rYREX"
   },
   "source": [
    "## Accuracy: MEAN BLEU  0.19103319326113646"
   ]
  }
 ],
 "metadata": {
  "accelerator": "TPU",
  "colab": {
   "collapsed_sections": [],
   "name": "Machine_Translation_Seq2Seq-Copy1.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
